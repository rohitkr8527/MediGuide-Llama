{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO8qr4NGmnHGkrrtl6wkQHc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rohitkr8527/MediGuide-Llama/blob/main/Mediguide_Llama.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sa9xqDs9mhwl",
        "outputId": "ad370c76-f3b1-4db3-aad7-e4ccce1a0e7f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 22 23:38:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0             28W /   70W |    5646MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17,
          "referenced_widgets": [
            "b5b7eb775230465cb94fe2e00249dfb3",
            "ee1af44135ce4371b97c217300823170",
            "ee9205e6cb72476696517e8f510c998b",
            "123763c9d51c4739815f063795634496",
            "4ff75e161e8640e3964aa73d88e5ecc5",
            "e7fa85a99b69431daab287415c7a9c4a",
            "f52da1b0a1f5491ebd0f69d54af0c955",
            "d6d9e41101a74987a457c6682baba181",
            "cc8492df4d334cdd9383f726959af0ea",
            "61a1fc9db9c14b06b74f03b9d48a8864",
            "cfc91f4f8e684ce2aae46cf51f28c3e6",
            "c1915275b69049d59e10e144677c026b",
            "fafd04d5fdc8468a94293971d29ebffb",
            "d52cc7273d494ce5961d7ddc81de0b1f",
            "8337bcfe0b5045cbb051838c473d4d84",
            "c0bf3cfcb6414fa0b962a91dc9de0aa8",
            "c2f8178e7ef14a6dac6262edf0d8cad8",
            "ce72b538b53c476484c69350150e582f",
            "b2c8aa8104614a0fb55f09c31b61384b",
            "99a175d291814494b6aa4e548b9ccd36"
          ]
        },
        "id": "ee97158c",
        "outputId": "77022ccf-c836-4819-c68a-a4782f380a1a"
      },
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "login()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv\u2026"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5b7eb775230465cb94fe2e00249dfb3"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "dC8fayhm0sSu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ba3f8193-263b-48da-b2dc-dd8c8bb2f009"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.21.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.5.1 which is incompatible.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install trl==0.11.4 transformers==4.45.0 peft==0.13.2 datasets==2.18.0 accelerate==0.27.2 bitsandbytes==0.43.1 torch>=2.1.0 triton==3.1.0"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip list | grep -E 'trl|transformers|peft|datasets|accelerate|bitsandbytes|torch|triton'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kfS6kwmkmfNl",
        "outputId": "db4e8c94-4d38-47d4-922c-348888f91361"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accelerate                            0.27.2\n",
            "bitsandbytes                          0.43.1\n",
            "datasets                              2.18.0\n",
            "fastrlock                             0.8.3\n",
            "peft                                  0.13.2\n",
            "sentence-transformers                 4.1.0\n",
            "tensorflow-datasets                   4.9.9\n",
            "torch                                 2.5.1\n",
            "torchao                               0.10.0\n",
            "torchaudio                            2.6.0+cu124\n",
            "torchdata                             0.11.0\n",
            "torchsummary                          1.5.1\n",
            "torchtune                             0.6.1\n",
            "torchvision                           0.21.0+cu124\n",
            "transformers                          4.45.0\n",
            "triton                                3.1.0\n",
            "trl                                   0.11.4\n",
            "vega-datasets                         0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "with open(\"HealthCareMagic-100k.json\", \"r\", encoding='utf-8') as f:\n",
        "    raw_data = json.load(f)\n",
        "\n",
        "dataset = Dataset.from_list([\n",
        "    {\n",
        "        \"prompt\": f\"### Instruction:\\n{entry['instruction']}\\n\\n### Input:\\n{entry['input']}\\n\\n### Response:\\n\",\n",
        "        \"response\": entry[\"output\"]\n",
        "    } for entry in raw_data\n",
        "])"
      ],
      "metadata": {
        "id": "a8Rc0cr-mlfN"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "model_id = \"meta-llama/Llama-3.1-8B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\"\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "\n",
        ")\n",
        "\n",
        "tokenizer.pad_token = tokenizer.eos_token\n"
      ],
      "metadata": {
        "id": "uub6j4hzmxFW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "676b62b1fcfe49cbbf3d925a62300a8c",
            "2b5b5697b1f24cef9c9f86e23789003a",
            "090bb96bd9e5451a9cf931b67334fc93",
            "3ef03bea9cf94c4fb56e186f38e32006",
            "67e4ff06046d4fb3b7fe590ef73ed5c9",
            "663c25d21cda490a8b3a41eaa9a8e987",
            "f0b544b347c14327a4fc59bda80f7dab",
            "84c66136785e431f86d4ceaf7a94d73d",
            "903b85c3c9674bc9a946068e23ad910f",
            "ade67b90dee448a685e22fb56912651e",
            "832fce96571f4d569bd95059ae97dc2f",
            "74a8eecfbb0240b48fb8b925f6c5dc55",
            "3ef22e400ee3417189fb00a1eb2af5e7",
            "545b767840b94152b3cc746c17b79cab",
            "553f00b1df3643358b2a979397529665",
            "aad95d5e462c46bb99da32a4b1df44a4",
            "3c0c652b9ded4ecba939728458ae0496",
            "e14810fee62a443bb9f5dfd90e686d9a",
            "57f9c248612a4cee8fd64d7110fbb5d4",
            "f07b138af6d4401bb534e8d922bfcaf5",
            "549f7aa015c54455a56e3fb348c5ccd2",
            "443d1050a545432885b3557f539ffb51",
            "a1f5a22c6a4146ab8841c0dc664a1344",
            "e61c542683624568999e04953955e1f7",
            "2d93d6229095493f8825508c17ade10b",
            "18f40f0942f542e9ab3adf752b183544",
            "2ff4badea7854dbf9f9fe7fbc7dfd617",
            "8b46b0c88a0448ab8a238b9b017a57f6",
            "cafd20d4a6b04939a9b859c02e69c178",
            "98729ea5e8554666830a79d72d1cf8fd",
            "618edeaee4714ce88c53841463073ff1",
            "4a583574a15842cba80497aa57c3f84d",
            "7ca7197f2f344554bae08dc1e646ac84",
            "50d99ca568624845858f05f1f78050ba",
            "690b989a8ddd49cc81b87426975ee5ab",
            "40661af4a8d04f80babd79e31a1f30d8",
            "1a921836d4dd4744bb4e88cc94861cac",
            "7b2b9c5d81eb450ab6161ef9f2d0f940",
            "14ccc61fe67744bc8ed247a9c6800588",
            "3a37cb7292714cf481344181cfc8930b",
            "53f03329184e45f7a7d6860bd693d6ae",
            "5c54e8b5e5114d8c923ec659c94e17d9",
            "092d7f1cfad34394ab7a2f290788d85e",
            "0b094cb84f83483088ad445cc42b2769",
            "f9ba796d02814e7f95e504b19e03052e",
            "614250d89e4c4ee89e293e4829f1a1e2",
            "9eaffdc64cea4ef19d031d3afe10e611",
            "800d02d41bca45508a580780fa704e4a",
            "f489519908074e6eba4fdd7bd8afbeb8",
            "6ee19268bb504ccfb6ef0ad178366d2d",
            "bd61d08bb7a1441199eb87935f6d61a2",
            "92ac505fedad48febc617e535a2ad1aa",
            "bfc6fe074d86499980a75a2f6a84e335",
            "76aeff7b519d490a889dfa7bff67ad75",
            "406f92906c5e4384a6c2826ad5afbf65",
            "8e006516c94245ba9173f9f13547218f",
            "e14e31a250704ef98eb5dbe348f403c5",
            "06bb688d2a2f41b5a89100d1fcf2aaad",
            "49f3f0a8122547abb006ce2f25339238",
            "cac54fc359424eeea51c4e27773a69f6",
            "1dd81cd19828492f953ff45ff2473936",
            "f73db19492ac4da7820eaad11dbd09d8",
            "fa6ed2e19778439b9a0441baa1aa0cba",
            "ff35c8f52c2f4183bcddb449361f9415",
            "a5d2e1b39a884e53b6e6032b83f751b8",
            "a67cbebb9edd4eebbf96b8f45b47de97",
            "90d2fc2a353741f0b0299575b50cc58a",
            "e07721624c2e453bbb97467ebd72c118",
            "ed12daa6ee8d4745b64789c3f5e691ac",
            "5a320d881e034f22816584ad1f8aada1",
            "628d0ffd4793449a95f9782f30d44700",
            "2f986a566e484cc2b254fa76033020fc",
            "567054f32cae4065a460a32b6cb3fe9f",
            "ecd4d149ec9e40b1a232ecbc1a59eb03",
            "3b112866d8bf475e8ab00d8279d9d2ea",
            "812dcb9f423146ad97de7bfb283dae08",
            "170c0d690a674b4fa442d2a6d8eee6ae",
            "5b57ba33494048a696f03848e8ee341b",
            "8b1ca2ec62d04b1e9aad9991204d704e",
            "cfbc7f5db10b4e3d8f2fececfde81d82",
            "2911c0c96c294c9c8c3d1071820458ad",
            "16cd54d912384521ae49a7bf92dfb815",
            "5989f12a3a594bd2bc7a8f5b2662fd58",
            "20eaa134cdaf4f2f85e38c982eeed629",
            "06c0550d861b4bbcb18dce3e5fd78b58",
            "e808b8c8cc1141089961c4ec36d06d0c",
            "6bd6c258b8d0418b9a25ee78e668c2f3",
            "9c2fd7378465492d9b7c1756d2062168",
            "9641ed3a0d5e48ea8b05347523dc2a87",
            "ccf042bee8784c2cafc63577e6e4670e",
            "f1261ec1be084272bee694ad4e4bcee4",
            "4732b0e0cdac4f259f118728bed0b993",
            "4c2760db73744cf3955568ba54f8a42f",
            "b3cb6b40472a4f91b17a857fe8f5248d",
            "98ab00048811402389e814110300e11b",
            "a7999f21f32348ba8509675a35a00d86",
            "a51724db50704896b8e1bd130383e7e8",
            "9a8da59771304ac38264d913985b1182",
            "4ec54aea198a4d1c952e6f9324267cd8",
            "d17ceae5521b442ca8451c63a4473eae",
            "5fc272cf758a4564b27d34a7ff303396",
            "35e613b28d8d4d489eee0515582a4249",
            "2cda29c010394592907f54844d210c44",
            "8258252c04a3490c892c6455ba5c8866",
            "3b0d9a4a9adc48809ca73d59609e429c",
            "5fe04a935d4648ee98bb4ab969a1bf3a",
            "f8f071e319914c5f94c9d264dfa2181c",
            "8dc6a246b0cb424e9f1625a449e2bf39",
            "81e1d1e8fd224891af721d1d4c151322",
            "4232538214414d93b838d310a9930039",
            "afe663ded0cc46e889df165b255364d1",
            "6210dddd42a443fc9988cd8806867c80",
            "b50f456627b543e9b8591e4564f2ae7a",
            "a36812a636d440a3b26f002924808139",
            "31fb5c90dbd54884a60631e4547d9acb",
            "db0f14b05c7749c99b54a22471a3c2a7",
            "265346ba7e074e2a8b179742dd8ee61f",
            "68758791fb504cebac8acd97581be707",
            "f0af611788554936a3b814af6a37d353",
            "1d0cfca66358485d8b9b45f834f5abbe",
            "28da8ac0a1654c7798a0599b030b319c",
            "e1222c26f67446d182198f211ebe28b8",
            "e7e11f2b9f8b4e7ca44fb8221412edaa",
            "80a79900955744e2a0ec5949a61cf5b4",
            "e375ed040d034f3bba4b2dc1bd0438d0",
            "e1a980955f234771a14471034c271987",
            "bc93c42556784c589dff82a0ee7b29f3",
            "26d3215d79a34619b9433a3edd462753",
            "d5921d0fd5ac46c093e1d035758a7100",
            "94e25ba13cd34c20a252abf3e9ba547c",
            "093b5c0cb1cf4eaebf8d156b7e172a02",
            "5d1e91bb1ec74f9db97f5f6746c1ef5c"
          ]
        },
        "outputId": "8d58a972-cce7-4915-912b-ee286d219704"
      },
      "execution_count": 1,
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "676b62b1fcfe49cbbf3d925a62300a8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/55.4k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "74a8eecfbb0240b48fb8b925f6c5dc55",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a1f5a22c6a4146ab8841c0dc664a1344",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50d99ca568624845858f05f1f78050ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/855 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f9ba796d02814e7f95e504b19e03052e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e006516c94245ba9173f9f13547218f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "90d2fc2a353741f0b0299575b50cc58a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b57ba33494048a696f03848e8ee341b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9641ed3a0d5e48ea8b05347523dc2a87"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d17ceae5521b442ca8451c63a4473eae"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afe663ded0cc46e889df165b255364d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/184 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e1222c26f67446d182198f211ebe28b8"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False"
      ],
      "metadata": {
        "id": "ECBjcfmgCmdG"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dV2xuP-zyU5K",
        "outputId": "a57233f1-d761-48db-b548-f05697d07630"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jun 22 23:38:13 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   51C    P0             28W /   70W |    5646MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "model.config.use_cache = False\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)"
      ],
      "metadata": {
        "id": "mHAG-PLLyXkk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForLanguageModeling\n",
        "\n",
        "def tokenize(example):\n",
        "    return tokenizer(\n",
        "        example[\"prompt\"] + example[\"response\"],\n",
        "        truncation=True,\n",
        "        max_length=512,\n",
        "        padding=\"longest\"\n",
        "    )\n",
        "\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize, remove_columns=[\"prompt\", \"response\"])\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "gakSYmAXyeVH",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "fae3321fefe241039772de309ca517d4",
            "326168598a7c4f47bb31bb42388bcce0",
            "5c581aafc47e4ae6bd6e5145c2ba5d3d",
            "9e560cd8835545db9e982742b038b255",
            "31b6d890988b41d594925e3dc8fc3dd4",
            "d5e793a78a2a4a3381072ae584950ab0",
            "51b7dca5883540288512e98a38e822f2",
            "48959e60c5e8460288063ce656d7b9f6",
            "4eccf604eeac45a385d62f4021325214",
            "e1943ef1c1104ef6973b257c9a6d4788",
            "af9ddc4d508e43b7a5f2bbffc6ce151b"
          ]
        },
        "outputId": "0d09cccd-19a8-4562-f6c0-d437a9c196ff"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/112165 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "fae3321fefe241039772de309ca517d4"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./mistral-lora-e-doctor\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    max_steps=1000,\n",
        "    learning_rate=2e-4,\n",
        "    logging_steps=20,\n",
        "    save_steps=500,\n",
        "    save_total_limit=1,\n",
        "    fp16=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "# STEP 6: Train\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QVN1b2Xwyqhf",
        "outputId": "5e724094-001c-4786-b2e1-5c93a31c1ef7"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/accelerate/accelerator.py:450: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
            "max_steps is given, it will override any value given in num_train_epochs\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\".*use_reentrant parameter should be passed explicitly.*\")"
      ],
      "metadata": {
        "id": "vfD5ioxvEATD"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "9XEd2gv-CHQE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"./mistral-lora-e-doctor\")\n",
        "tokenizer.save_pretrained(\"./mistral-lora-e-doctor\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zaLyi3nERSc",
        "outputId": "a5a1472e-c496-4c5a-8396-5f8e1e5b37d7"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./mistral-lora-e-doctor/tokenizer_config.json',\n",
              " './mistral-lora-e-doctor/special_tokens_map.json',\n",
              " './mistral-lora-e-doctor/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5t-wJj4z7W2",
        "outputId": "2318ad12-edf1-437a-d7f0-d1e0ba4d9bc7"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained('/content/drive/My Drive/mistral-lora-e-doctor')\n",
        "tokenizer.save_pretrained('/content/drive/My Drive/mistral-lora-e-doctor')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9044qfzc0lpx",
        "outputId": "f6822b12-3871-4219-ee5f-3afd2a023faf"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/mistral-lora-e-doctor/tokenizer_config.json',\n",
              " '/content/drive/My Drive/mistral-lora-e-doctor/special_tokens_map.json',\n",
              " '/content/drive/My Drive/mistral-lora-e-doctor/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-_E9dg7K1WE5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}